## 数据集成：这些大号一共20亿粉丝？ 

## 数据集成的两种构架：ELT和ETL 

# 数据变换：考试成绩要求正态分布合理吗？

## 数据变换在数据分析中的角色

数据变换是数据准备的重要环节，*它通过平滑、数据聚集、数据概化和规划化等方式*将数据转换成适用于数据挖掘的形式

常见的变换方法：

1.数据平滑：去除数据中的噪声，将连续数据离散化。可以采用分箱、聚类、和回归的方式进行数据平滑

2.数据聚集：对数据进行汇总，在SQL中有一些聚集函数可以提供我们操作，比如Max（）、Sum（）

3.数据概化：将数据由较低的概念抽象成为较高的概念，减少数据复杂程度，既用更高的概念替代更低的概念。

4.数据规划化：使属性数据按比列缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小-最大规范法、Z-score规范化、按小数定标规划化

5.属性构造：构造出新的属性并添加到属性集中，这里会用到特征工程知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程

### 数据规范化的集中方法

1.Min-max规范化
公式： 新数值 =（原数值-级小值）/（极大值-极小值）

2.Z-Score规范化

公式： 新数值 =（原数值 - 均值/标准差

3.小数定标规划化

公式：新数值 = 原数值/1000

#### Python的SciKit-Learn库使用

SciKit-Learn库是Python的重要机器学习库，它帮我们封装了大量的机器学习算法，比如分类、聚类、回归、降维等。还包括数据变换模块

1.Min-max 规范化

```python
# coding:utf-8
from sklearn import preprocessing
import numpy as np
# 初始化数据，每一行表示一个样本，每一列表示一个特征
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行 [0,1] 规范化
min_max_scaler = preprocessing.MinMaxScaler()
minmax_x = min_max_scaler.fit_transform(x)
print(minmax_x)

```

2.Z-Score规范化
```python
from sklearn import preprocessing
import numpy as np
# 初始化数据
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行 Z-Score 规范化
scaled_x = preprocessing.scale(x)
print scaled_x
```

*实际上就是每行每列的值减去了平均值，再除以方差的结果*
符合均值为0，方差为1的正太分布

3.小数定标规范化
```python
# coding:utf-8
from sklearn import preprocessing
import numpy as np
# 初始化数据
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 小数定标规范化
j = np.ceil(np.log10(np.max(abs(x))))
scaled_x = x/(10**j)
print scaled_x
```

#### 数据挖掘中数据转换比算法选择更重要